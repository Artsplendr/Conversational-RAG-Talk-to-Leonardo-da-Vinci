{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "293f8d2e",
      "metadata": {
        "id": "293f8d2e"
      },
      "source": [
        "# Conversational RAG: Talk to Leonardo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational-RAG Introduction\n",
        "\n",
        "Conversational Retrieval-Augmented Generation (RAG) is an advanced architecture designed to enhance natural language understanding in multi-turn dialog systems. Unlike basic RAG, which retrieves relevant documents and generates responses in a single-turn format, Conversational RAG incorporates memory‚Äîenabling the system to maintain context across multiple exchanges. This is particularly powerful for applications like virtual assistants, tutoring bots, or expert agents, where user queries often reference previous interactions. By combining long-term memory with intelligent retrieval, Conversational RAG ensures that responses are not only grounded in external knowledge but are also coherent with the ongoing conversation.\n",
        "\n",
        "The architecture typically consists of a vector database to store embedded documents, a memory module to track conversational history, and a large language model (LLM) to synthesize responses. During each interaction, the model uses both retrieved documents and the dialogue memory to generate answers that are contextually aware and knowledge-grounded. Frameworks like LlamaIndex and LangChain provide ready-to-use components to build such systems, enabling construction of robust and intelligent agents that feel more human-like and contextually fluent."
      ],
      "metadata": {
        "id": "K2eXtpbaLCog"
      },
      "id": "K2eXtpbaLCog"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goal\n",
        "\n",
        "The goal of this project is to create a \"coversation\" with a historical \"person\", by using Conversational RAG to pull real quotes and references from their speeches, letters, and books. The memory enables multi-turn interactions that simulate a natural conversation.\n",
        "\n",
        "In this example, there will be \"conversation\" with Leonardo da Vinci, by using Conversational RAG to extract qoutes and references from his work [\"Thoughts on Art and Life\".](https://www.gutenberg.org/ebooks/29904)"
      ],
      "metadata": {
        "id": "62P-ZbGmLJ3O"
      },
      "id": "62P-ZbGmLJ3O"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vtQddpYYVH2s"
      },
      "id": "vtQddpYYVH2s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"your-path-here\""
      ],
      "metadata": {
        "id": "SH142llJVKu6"
      },
      "id": "SH142llJVKu6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6404e705",
      "metadata": {
        "id": "6404e705"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%%capture\n",
        "!pip install -q langchain langchain_community openai chromadb tiktoken\n",
        "!pip install -U langchain-openai\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "91d4d5aa",
      "metadata": {
        "id": "91d4d5aa"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "%%capture\n",
        "import os\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from IPython.display import display, HTML, Image\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import VBox, HTML\n",
        "from google.colab import output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\""
      ],
      "metadata": {
        "id": "ySkdOPaCVRds"
      },
      "id": "ySkdOPaCVRds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and split document\n",
        "loader = TextLoader(\"29904.txt\")  # Make sure the text file is loaded\n",
        "raw_docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "documents = splitter.split_documents(raw_docs)"
      ],
      "metadata": {
        "id": "RTp_SbExvfbc"
      },
      "id": "RTp_SbExvfbc",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vectorstore from documents\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embedding=embedding)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "MnkjIp0IvfeC"
      },
      "id": "MnkjIp0IvfeC",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup memory and prompt template\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
      ],
      "metadata": {
        "id": "kwibXm5Evfgi"
      },
      "id": "kwibXm5Evfgi",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are Leonardo da Vinci, speaking as a wise and reflective artist based only on your writings in *Thoughts on Art and Life*.\n",
        "Your tone is insightful, poetic, and gently philosophical ‚Äî but not overly repetitive or formal.\n",
        "Please avoid starting every reply the same way. Vary your introductions naturally and do not use the word \"apprentice.\"\n",
        "\n",
        "Use only the context provided and stay grounded in the historical style.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer as Leonardo:\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "r-3YrlMfxmI5"
      },
      "id": "r-3YrlMfxmI5",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define chain\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ],
      "metadata": {
        "id": "4EQbftShvfjW"
      },
      "id": "4EQbftShvfjW",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format Output Blocks\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "def format_block(role, text):\n",
        "    # Assign emoji: üéì for Apprentice, üßë‚Äçüé® for Leonardo\n",
        "    emoji = \"üßë‚Äçüé®\" if role == \"Leonardo\" else \"üéì\"\n",
        "\n",
        "    # Format text into paragraphs\n",
        "    content_html = \"<br><br>\".join(p.strip() for p in text.split(\"\\n\") if p.strip())\n",
        "\n",
        "    # HTML block with styling\n",
        "    html = f\"\"\"\n",
        "    <div style=\"max-width:900px; padding:12px 24px; margin:12px 0;\n",
        "                background-color:#fff; border-radius:10px;\n",
        "                box-shadow: 0 4px 12px rgba(0,0,0,0.05); font-family:Georgia,serif;\n",
        "                font-size:15px; line-height:1.6; text-align:left\">\n",
        "        <b>{emoji} {role}:</b><br><br>\n",
        "        {content_html}\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return widgets.HTML(value=html)"
      ],
      "metadata": {
        "id": "Nqfzrf6PK0Y2"
      },
      "id": "Nqfzrf6PK0Y2",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UI Setup\n",
        "chat_display = widgets.VBox(layout=widgets.Layout(align_items=\"flex-start\"))\n",
        "\n",
        "input_box = widgets.Textarea(\n",
        "    placeholder=\"Ask Leonardo something...\",\n",
        "    layout=widgets.Layout(width=\"1000px\", height=\"60px\")\n",
        ")\n",
        "\n",
        "submit_button = widgets.Button(\n",
        "    description=\"Ask Leonardo\",\n",
        "    button_style=\"primary\",\n",
        "    layout=widgets.Layout(width=\"130px\", height=\"40px\")\n",
        ")\n",
        "\n",
        "def on_submit(_=None):\n",
        "    question = input_box.value.strip()\n",
        "    if question:\n",
        "        chat_display.children += (format_block(\"Apprentice\", f\"<i>{question}</i>\"),)\n",
        "        input_box.value = \"\"\n",
        "        answer = qa_chain.run(question)\n",
        "        chat_display.children += (format_block(\"Leonardo\", answer),)\n",
        "\n",
        "submit_button.on_click(on_submit)\n"
      ],
      "metadata": {
        "id": "n0oKdOUYL4v8"
      },
      "id": "n0oKdOUYL4v8",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layout & Display\n",
        "header = widgets.HTML(\n",
        "    value=\"\"\"\n",
        "    <h2 style=\"font-family:Georgia,serif; text-align:center; padding:10px\">\n",
        "        Ask Leonardo da Vinci anything from <em>Thoughts on Art and Life</em>\n",
        "    </h2>\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "input_area = widgets.HBox([input_box, submit_button])\n",
        "app_layout = widgets.VBox(\n",
        "    [header, chat_display, input_area],\n",
        "    layout=widgets.Layout(width=\"100%\", align_items=\"flex-start\")  # Align left\n",
        ")\n",
        "\n",
        "display(app_layout)"
      ],
      "metadata": {
        "id": "AzvsriLTlQow"
      },
      "id": "AzvsriLTlQow",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}